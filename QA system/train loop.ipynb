{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "known-hearts",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import re\n",
    "import datasets\n",
    "from nltk.corpus import stopwords\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "\n",
    "def unpack_data():    \n",
    "    with jsonlines.open('test.jsonl') as reader1, jsonlines.open('train.jsonl') as reader2, jsonlines.open('val.jsonl') as reader3:\n",
    "        test_data = construcked_data(reader1)\n",
    "        train_data = construcked_data(reader2)\n",
    "        val_data = construcked_data(reader3)\n",
    "        return test_data, train_data, val_data\n",
    "              \n",
    "def construcked_data(data):\n",
    "        train_data={}\n",
    "        questions = []\n",
    "        answer_labeled = []\n",
    "        tmpDict=[]\n",
    "        for deep in data:     \n",
    "            for deeper in deep.get('passage').get('questions'):    \n",
    "                for deepest in deeper['answers']:\n",
    "                    if \"label\" in deepest and deepest.get('label') == 1:\n",
    "                        answer_labeled.append({\"answer\": deepest.get('text'), \"label\": deepest.get('label')})\n",
    "                    elif \"label\" not in deepest:\n",
    "                        answer_labeled.append({\"answer\": deepest.get('text')})\n",
    "                questions.append({\"question\": deeper.get('question'), \"answers\": answer_labeled})\n",
    "                answer_labeled= []\n",
    "            passage = deep.get('passage').get('text')\n",
    "            passage = re.sub(r'\\([-+]?\\d+\\)', '', passage)\n",
    "            tmpDict.append({\"idx\": deep.get('idx'), \"passage\": passage , \"questions\": questions})      \n",
    "            questions=[] \n",
    "        \n",
    "        return(tmpDict)\n",
    "        \n",
    "test_data, train_data, val_data = unpack_data()\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def find_start_pos(data):\n",
    "    \n",
    "    morph = MorphAnalyzer()\n",
    "    russian_stopwords = stopwords.words(\"russian\")\n",
    "    \n",
    "    def token_analizer(idx,token_passage,token_answer):     \n",
    "        indexes_ans = []\n",
    "        startpoz = 0\n",
    "        endpoz = 0\n",
    "        \n",
    "        indexes_ans.append([i for i, x in enumerate(token_passage) if x == token_answer[0]])\n",
    "        indexes_ans.append([i for i, x in enumerate(token_passage) if x == token_answer[-1]])\n",
    "        if len(indexes_ans[0])>0 and len(indexes_ans[-1])>0:\n",
    "            for j in indexes_ans[0]:\n",
    "                for d in indexes_ans[-1]:\n",
    "                    if len(token_answer)-2 < d-j and len(token_answer)+3 > d-j:\n",
    "                        startpoz = j\n",
    "                        endpoz = d\n",
    "                    break\n",
    "                \n",
    "                    \n",
    "                    \n",
    "                  \n",
    "            \n",
    "        \n",
    "        #print(token_passage,token_answer)\n",
    "        #print(indexes_ans) \n",
    "        #print(startpoz)\n",
    "        #print(idx)\n",
    "        return startpoz, endpoz\n",
    "        \n",
    "    def make_token(text):\n",
    "        tokened = []\n",
    "        \n",
    "        for token in text.split():\n",
    "            token = token.strip().strip('.')\n",
    "            token = morph.normal_forms(token)[0]\n",
    "            tokened.append(token)\n",
    "        \n",
    "        return tokened\n",
    "    \n",
    "    def untokened_label(text, label_s, label_e):\n",
    "        if label_s != 0:\n",
    "            tokened = []\n",
    "            tx = 0\n",
    "            tx2 = 0\n",
    "            for token in text.split():\n",
    "                tokened.append(token)\n",
    "            for i in range(label_s):\n",
    "                tx+=(len(tokened[i])+1)\n",
    "            for i in range(label_e):\n",
    "                tx2+=(len(tokened[i])+1)\n",
    "            index = text.find(tokened[label_s], tx)\n",
    "            index1 = text.find(tokened[label_s], tx, tx2)\n",
    "            if index == index1 or index1 == -1:\n",
    "                return index\n",
    "            \n",
    "            \n",
    "        \n",
    "        else:\n",
    "            return None\n",
    "            \n",
    "    \n",
    "    \n",
    "    \n",
    "    for deep in data:\n",
    "        \n",
    "        passage = deep.get('passage')\n",
    "        \n",
    "        passage = re.sub(r'[^\\w\\s]','', passage) \n",
    "        token_passage = make_token(passage)\n",
    "        for deeper in deep.get('questions'):\n",
    "            for deepest in deeper['answers']:\n",
    "                answer = re.sub(r'[^\\w\\s]','', deepest.get('answer')) \n",
    "                token_answer=make_token(answer)\n",
    "                st, ed = token_analizer(deep.get('idx'),token_passage,token_answer)\n",
    "                label = untokened_label(deep.get('passage'),st, ed)\n",
    "                deepest['label'] = label\n",
    "                \n",
    "    \n",
    "        token_passage=[]\n",
    "    \n",
    "    \n",
    "        \n",
    "                    \n",
    "    \n",
    "find_start_pos(train_data)\n",
    "\n",
    "find_start_pos(val_data)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "novel-appendix",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 384\n",
    "stride = 128\n",
    "\n",
    "\n",
    "def preprocess_training_examples(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = answers[sample_idx]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        if answer[\"answer_start\"][0] != None:\n",
    "            \n",
    "            end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "            sequence_ids = inputs.sequence_ids(i)\n",
    "    \n",
    "            # Find the start and end of the context\n",
    "            idx = 0\n",
    "            while sequence_ids[idx] != 1:\n",
    "                idx += 1\n",
    "            context_start = idx\n",
    "            while sequence_ids[idx] == 1:\n",
    "                idx += 1\n",
    "            context_end = idx - 1\n",
    "    \n",
    "            # If the answer is not fully inside the context, label is (0, 0)\n",
    "            if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "                start_positions.append(0)\n",
    "                end_positions.append(0)\n",
    "            else:\n",
    "                # Otherwise it's the start and end token positions\n",
    "                idx = context_start\n",
    "                while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                    idx += 1\n",
    "                start_positions.append(idx - 1)\n",
    "    \n",
    "                idx = context_end\n",
    "                while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                    idx -= 1\n",
    "                end_positions.append(idx + 1)\n",
    "        else:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "                  \n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs\n",
    "\n",
    "def preprocess_validation_examples(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    example_ids = []\n",
    "    \n",
    "    for i in range(len(inputs[\"input_ids\"])):\n",
    "        sample_idx = sample_map[i]\n",
    "        example_ids.append(examples[\"id\"][sample_idx])\n",
    "\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        offset = inputs[\"offset_mapping\"][i]\n",
    "        inputs[\"offset_mapping\"][i] = [\n",
    "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "        ]\n",
    "\n",
    "    inputs[\"example_id\"] = example_ids\n",
    "    return inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "minus-amplifier",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.010003805160522461,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "Map",
       "rate": null,
       "total": 5380,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ed8d1df178746c188c762912542a6d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5380 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0110015869140625,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "Map",
       "rate": null,
       "total": 993,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dda3141a249c425d94c5fad1c8a2935e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/993 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset\n",
    "trained_checkpoint = \"timpal0l/mdeberta-v3-base-squad2/checkpoint-3603\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(trained_checkpoint)\n",
    "ans = str\n",
    "label = []\n",
    "a=[]\n",
    "idx = 0\n",
    "for deep in val_data:     \n",
    "        for deeper in deep.get('questions'):        \n",
    "            for deepest in deeper['answers']:\n",
    "                ans=deepest.get('answer')\n",
    "                label= deepest.get('label')\n",
    "                \n",
    "                a.append({'id':str(idx),'context':deep.get('passage'),'question':deeper.get('question'), 'answers':ans, 'answer_start':label})\n",
    "                idx+=1\n",
    "                ans = str\n",
    "                label = str\n",
    "\n",
    "valid_dataset_premade = Dataset.from_list(a)\n",
    "\n",
    "\n",
    "\n",
    "ans = []\n",
    "label = []\n",
    "a=[]\n",
    "idx = 0\n",
    "for deep in train_data:     \n",
    "        for deeper in deep.get('questions'):        \n",
    "            for deepest in deeper['answers']:\n",
    "                ans.append(deepest.get('answer'))\n",
    "                label.append(deepest.get('label'))\n",
    "                a.append({'id':str(idx),'context':deep.get('passage'),'question':deeper.get('question'), 'answers':{\"text\":ans , 'answer_start':label}})\n",
    "                ans = []\n",
    "                label = []\n",
    "                idx+=1\n",
    "\n",
    "train_dataset_pre = Dataset.from_list(a)\n",
    "\n",
    "\n",
    "train_dataset = train_dataset_pre.map(\n",
    "    preprocess_training_examples,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset_pre.column_names,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "eval_set = valid_dataset_premade.map(\n",
    "    preprocess_validation_examples,\n",
    "    batched=True,\n",
    "    remove_columns = valid_dataset_premade.column_names,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "judicial-victorian",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import collections\n",
    "metric = evaluate.load(\"squad\")\n",
    "n_best = 20\n",
    "max_answer_length = 30\n",
    "theoretical_answers = []\n",
    "predicted_answers = []\n",
    "\n",
    "def compute_metrics(start_logits, end_logits, features, examples):\n",
    "    example_to_features = collections.defaultdict(list)\n",
    "    for idx, feature in enumerate(features):\n",
    "        example_to_features[feature[\"example_id\"]].append(idx)\n",
    "\n",
    "    predicted_answers = []\n",
    "    for example in tqdm(examples):\n",
    "        example_id = example[\"id\"]\n",
    "        context = example[\"context\"]\n",
    "        answers = []\n",
    "\n",
    "        # Loop through all features associated with that example\n",
    "        for feature_index in example_to_features[example_id]:\n",
    "            start_logit = start_logits[feature_index]\n",
    "            end_logit = end_logits[feature_index]\n",
    "            offsets = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Skip answers that are not fully in the context\n",
    "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                        continue\n",
    "                    # Skip answers with a length that is either < 0 or > max_answer_length\n",
    "                    if (\n",
    "                        end_index < start_index\n",
    "                        or end_index - start_index + 1 > max_answer_length\n",
    "                    ):\n",
    "                        continue\n",
    "\n",
    "                    answer = {\n",
    "                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
    "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                    }\n",
    "                    answers.append(answer)\n",
    "\n",
    "        # Select the answer with the best score\n",
    "        if len(answers) > 0:\n",
    "            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "            predicted_answers.append(\n",
    "                {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]}\n",
    "            )\n",
    "        else:\n",
    "            predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n",
    "    for ex in examples:\n",
    "        theoretical_answers.append({\"id\": ex[\"id\"], \"answers\":[{\"text\": ex[\"answers\"], \"answer_start\": ex['answer_start']}]}) \n",
    "    print(len(theoretical_answers[0]['answers']))\n",
    "    return metric.compute(predictions=predicted_answers, references=theoretical_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concerned-guitar",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "import torch\n",
    "from transformers import AutoModelForQuestionAnswering\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import Trainer\n",
    "\n",
    "model_checkpoint = \"timpal0l/mdeberta-v3-base-squad2/checkpoint-3603\"\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"timpal0l/mdeberta-v3-base-squad2/checkpoint-3603\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    \n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_set,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "predictions, _, _ = trainer.predict(eval_set)\n",
    "start_logits, end_logits = predictions\n",
    "\n",
    "compute_metrics(start_logits, end_logits, eval_set, valid_dataset_premade)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genetic-aruba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complex-chicago",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
